- Upstage에서 강의한 파라미터 수가 많은 거대한 언어 모델을 빠르고 효과적으로 학습하는 방법인 MoE(Mixed Of Experts) 개념 정리 1일차

- Microsoft에서 공개한 MoE를 사용하여 GPT3에 버금가는 고성능의 다국어 모델 review 
    - Scalable and Efficient MoE Training for Multitask Multilingual Models

- Microsoft의 GPU 분산 학습 라이브러리 DeepSpeed 사용법 튜토리얼 정리(진행중)
    - toy project 및 업무에서 사용가능하게 boilerplate로 class code 작성
    - 실제 huggingFace의 분산처리 학습 라이브러리 Accelerator와의 비교 및 장단점 살펴보기

