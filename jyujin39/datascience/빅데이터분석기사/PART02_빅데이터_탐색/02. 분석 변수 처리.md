# 02 분석 변수 처리

## 1. 변수 선택

통계분석의 신뢰성을 위해서는 기본적으로 데이터와 변수가 많으면 많을수록 좋지만, 너무 많으면 분석모형을 구성하고 유지하는 데에 많은 비용이 들기 때문에, 어느정도의 설명력이 유지되는 선에서는 변수를 적게 선택하는 것이 효율적이다.

변수를 선택하는 다양한 방법들을 알아보자.

<br>

### 1) 변수별 모형의 분류

- 전체 모형(FM: Full Model): 모든 독립변수를 사용한 모형
- 축소 모형(RM: Reduced Model): 변수의 개수를 축소한 모형
- 영 모형(NM: Null Model): 독립변수가 하나도 없는 모형

<br>

### 2) 변수의 선택 방법

참고자료: https://zephyrus1111.tistory.com/65

- 전진 선택법(Forward Selection)
  - 영모형에서 시작해, 모든 독립변수 중 종속변수와 단순상관계수의 절댓값이 가장 큰 변수부터 하나씩 분석모형에 포함시킨다.
  - 부분 F검정(F test)를 통해 유의성 검증을 시행하여, 유의한 경우는 F통계량이 가장 큰 모형을 선택하고 유의하지 않은 경우 변수를 더 이상 추가하지 않고 과정을 중단.
  - 한 번 추가된 변수는 제거하지 않는다.
- 후진 선택법(Backward Selection), 후진 소거법(Backward Elimination)
  - 전체모형에서 시작해, 모든 독립변수 중 종속변수와 단순상관계수의 절댓값이 가장 작은 변수부터 하나씩 분석모형에서 제외시킨다.
  - 부분 F검정을 통해 유의성 검증을 시행하여, 유의하지 않은 경우는 변수를 제거하고 유의한 경우는 변수를 더 이상 제거하지 않고 과정을 중단.
  - 한 번 제거된 변수는 추가하지 않는다.
- 단계적 선택법(Stepwise Selection)
  - 전진 선택법과 후진 선택법의 보완방법.
  - 전진 선택법을 통해 유의한 변수들을 모형에 포함한 후, 선택된 변수들에 대해 다시 후진 선택법을 적용하여 중요하지 않은 변수를 제거하는 과정을 반복한다.
  - 더 이상 추가하거나 제거할 변수가 없을 때 종료한다.

<br><br>

## 2. 차원 축소

**차원**이란 데이터의 종류(변수)를 의미한다. 차원을 축소한다는 것은 어떤 목적에 따라서 변수의 양을 줄이는 것을 의미한다.

### 1) 차원 축소의 필요성

- 복잡도의 축소(Reduce Complexity)
  - 변수가 많아지면 분석시간의 증가(시간복잡도: Time Complexity)와 저장변수 양의 증가(공간복잡도: Space Complexity) 문제 발생
  - 동일한 품질을 나타낼 수 있다면 효율성 측면에서 차원의 수를 줄여야 한다.
- 과적합(Overfit)의 방지
  - 차원이 많아지면 분석모델의 파라미터 증가 및 파라미터간의 관계 증가로 과적합 발생의 가능성이 커진다.
  - 이는 분석모델 정확도(신뢰도) 저하로 이어진다
  - 적은 차원으로 안정적인(robust) 결과를 낼 수 있다면 많은 차원을 다루는 것보다 효율적이다
- 해석력(Interpretability)의 확보
  - 차원 수가 적은 간단한 모델일수록 해석 및 설명이 쉬워짐
- 차원의 저주(Curse of Dimensionality) 방지
  - **차원의 저주**란, **학습데이터의 수(데이터셋 개수)보다 차원의 수(변수 개수)가 많아져 성능이 저하되는 현상**을 의미
  - 차원을 줄이거나 데이터 수를 늘려야 함

<br>

### 2) 차원 축소의 방법

#### (1) **요인 분석(Factor Analysis)**

- 요인 분석의 개념

  - **변수들간의 상관관계를 분석하여 공통된 차원을 축약**하는 통계분석 과정.

- 요인분석의 목적

  - 변수 축소: 정보손실을 최대한 억제하면서 소수의 요인(Factor)으로 축약할 수 있음
  - 변수 제거: 요인에 대한 중요도를 파악할 수 있음
  - 변수 특성 파악: 서로 상관된 변수들이 묶임으로써 요인간의 상호 독립성 파악이 용이해짐

  - 타당성 평가: 다른 변수들과 묶이지 않는 변수의 독립성 여부를 판단할 수 있음
  - 파생변수: 요인점수를 이용해 새로운 변수를 생성할 수 있음

- 요인분석의 특징

  - 독립변수, 종속변수 개념이 없음. 주로 기술통계에 의한 방법을 이용

- 요인분석의 종류

  - 주성분 분석(PCA), 공통요인 분석 특이값 분해(SVD: Singular Value Decomposition) 행렬, 음수미포함 행렬분해(NMF: Non-negative Matrix Factorization) 등
    - 공통요인 분석은 분석대상 변수들의 기저를 이루는 구조를 정의하기 위한 방법으로 변수들이 가지고있는 공통분산을 이용하여 공통요인만 추출하는 방법.

<br>

#### (2) 주성분 분석(PCA)

- 주성분 분석의 개념

  - **데이터들의 특성을 설명할 수 있는 하나 이상의 특징(주성분, Principal Component)를 찾는 것.**
  - 서로 연관성이 있는 고차원공간의 데이터를 선형연관성이 없는 저차원(주성분)으로 변환. (직교변환)
  - 기존의 기본 변수들을 세로운 변수세트로 변환해 차원을 줄이되, **기존 변수들의 분포 특성을 최대한 보존**하여 이를 통한 분석결과의 신뢰성을 확보

- PCA 방법의 이해

  - 아래 데이터를 가장 잘 설명하는 두 개의 벡터는 아래 파란 선과 붉은 선이다. 두 벡터의 방향과 크기를 알면 이 데이터 분포가 어떤 형태인지를 가장 단순하면서도 효과적으로 파악할 수 있다.
  - PCA는 데이터 포인트 하나하나의 성분을 분석하는 것이 아닌, 여러 **데이터들의 분포에 대한 주성분을 분석**하는 방법이다.

  ![image](https://user-images.githubusercontent.com/44221498/140925037-63b032a2-a78d-40a2-a573-ca8daf49c608.png)

<center>이미지 출처: https://enjoyso.tistory.com/104</center>

- PCA의 특징
  - 차원축소에 폭넓게 사용됨. 어떤 **사전적 분포 가정의 요구가 없음**
  - 가장 큰 분산의 방향들이 주요 관심
  - **변수들의 선형결합**으로만 고려함
  - **본래의 변수들이 서로 상관이 있을 때만 가능**
  - 스케일 영향이 큼. 즉 PCA 수행을 위해서는 **변수들의 스케일링 작업이 필수**

<br>

#### (3) 특이값 분해(SVD: Singular Value Decomposition)

- 특이값 분해의 개념

  - m x n 크기의 행렬 M을 다음과 같이 분해할 수 있다
    $$
    M = U\Sigma V^{T}
    $$

    - U - m x m 크기의 직교행렬(Orthogonal Matrix)

    - $\Sigma$​ - m x n 크기의 대각행렬(Diagonal Matric)

    - $V^T$​​​ - n x n 크기의 직교행렬

      <br>

  - 직교행렬의 정의

    - 행렬의 열백터 서로 가 독립인 행렬

    > U가 직교행렬이면      $UU^T = U^TU = I_m$      ($I_m$은 단위행렬: 대각성분이 모두 1인 행렬)
    >
    > 즉, $U^T = U^{-1}$​ 

  - 대각행렬의 정의

    - 행렬의 대각성분을 제외한 나머지 원소 값이 모두 0인 행렬.

    <br>

- 특이값 분해의 차원 축소 원리

  - 수학적 원리

    - 주어진 행렬 M(m x n)을 여러 개의 행렬로 분해할 수 있으며, 각 행렬의 원소값의 크기는 Diagonal Marix의 대각성분 크기에 의해 결정된다.

  - 데이터에 적용

    - **모든 차원을 보유한 기존 데이터 A를 SVD에 의해 3개의 행렬로 분해하면, 특이값 k만을 이용해 원래 데이터(행렬) A와 비슷한 정보력을 가지는 차원을 만들어낼 수 있다.**

    - 특이값 분해는 분해과정보다 분해된 행렬을 다시 조합하는 과정에서 더욱 의미가 있는데, $U, \Sigma, V^T$ 로 분해된 A행렬을 특이값 k개만을 이용해 $A'$​ 행렬로 부분복원할 수 있다. 특이값의 크기에 따라 데이터가 가진 정보량이 결정되기 때문에 값이 큰 몇 개의 특이값들을 가지고도 충분히 유용한 정보를 유지할 수 있다.

      (참조: https://angeloyeo.github.io/2019/08/01/SVD.html)

      ![image](https://user-images.githubusercontent.com/44221498/141254668-4b49ad1c-dce1-49e5-a244-a1a7e8fc9129.png)

<center>
  SVD를 통해 얻어진 분해행렬에서 일부만을 이용해 적당한 정보를 가진 A'를 만들어내는 과정
</center>

<br>

#### (4) 음수 미포함 행렬분해(NMF: Non-negative Matrix Factorization)

음수를 포함하지 않은 행렬 V를 음수를 포함하지 않은 두 행렬 W와 H의 곱으로 분해하는 알고리즘.
$$
V = WH
$$


- NMF의 이해

  - 기존 행렬 V와 분해한 음수 미포함 행렬 W, H의 곱과의 차이를 오차 U라고 한다. 즉, $V = WH + U$ (U의 원소들은 양수나 음수가 될 수 있다)

  - W와 H의 크기가 V보다 작기 때문에 저장하거나 다루기 용이하며, V를 원래 정보보다 상대적으로 적은 정보로 표현할 수 있다.

- NMF의 유용성
  (참조: https://angeloyeo.github.io/2020/10/15/NMF.html)

  - **NMF가 유용한 이유 중 하나는 W와 H 원소가 모두 non-negative 이기 때문**
    - 우리가 다루는 데이터 중 이미지와 같은 데이터에는 음수 값이 전혀 포함되지 않는다 (ex. 색상 값 0~255)
      그런데 많은 경우 사용되는 행렬 분해 과정은 획득되는 feature들이 음수이면 안된다는 제한사항이 없기 때문에 원래 데이터 특성인 non-negativity를 보존할 수 있는 보장이 없다
  - **NMF는 PCA나 SVD와 같은 차원축소 방법에 비해 데이터 구조를 더 잘 반영한다**
    - PCA나 SVD는 feature들 간의 직교성이 보장된다.
      그러나 feature 벡터들이 늘 서로 직교하게 되면 데이터셋의 실제 구조를 잘 반영하지 못할 수도 있다.

<br><br>

## 3. 파생변수의 생성

데이터 웨어하우스로부터 복제되었거나 자체수집된 데이터모임의 중간층에 해당하는 데이터마트에는 분석을 위한 기본단계변수가 모여있다.

이 기본 변수들에는 요약변수와 파생변수들이 있다.

### 1) 파생변수

- 기존 변수를 조합하여 새로운 변수를 만들어내는 것.
- 사용자가 값을 만들어 의미를 부여하는 변수로 매우 주관적일 수 있으므로 논리적 타당성을 갖출 필요가 있다
- 세분화 고객행동예측, 캠페인반응예측 등에 활용할 수 있다
- 특정상황에만 유의미하지 않도록 대표성을 나타내게 할 필요가 있음

### 2) 요약변수

- 수집된 정보를 분석목적에 맞게 종합(aggregate)한 변수
- 데이터마트에서 가장 기본적인 변수.
- 많은 분석모델에서 공통사용할 수 있어 재활용성이 높다

### 3) 요약변수 vs 파생변수

- 예시를 통한 이해

  - 고객관계관리(CRM: Customer Relationship Management) 데이터

    | 요약변수(단순 종합 개념) | 파생변수(주관적 변수 개념) |
    | :----------------------: | :------------------------: |
    |      매장이용 횟수       |      주 구매매장 변수      |
    |    구매상품품목 개수     |    구매상품 다양성 변수    |
    | 기간별 구매금액 및 횟수  |      주 활동지역 변수      |
    | 상품별 구매금액 및 횟수  |      주 구매상품 변수      |

    

  - 요약변수 처리 시 유의점

    - 단어의 빈도, 초기행동변수, 트렌드변수 등 처리방법에 따라 결측값 및 아웃라이어 처리에 유의해야 함
    - 연속형 변수의 구간화와 그를 통한 의미 파악 시 정구간이 아닌 의미 있는 구간을 찾도록 해야 함

  - 파생변수 생성 및 처리 시 유의점

    - 특정 상황에서만 의미있지 않고 보편적이고 대표성을 갖는 파생변수를 생성해야 함
    - 생성 방법 예시
      - 한 값으로부터 특징 추출
      - 한 레코드 내의 값들을 결합
      - 다른 테이블의 부가적 정보를 결합
      - 다수의 필드 내에 시간 종속적인 데이터를 선택
      - 레코드 또는 중요필드를 요약

<br><br>

## 4. 변수 변환

### 1) 변수 변환의 개념

- **데이터를 분석하기 좋은 형태로 바꾸는 작업**
- 데이터의 전처리 과정 중 하나로 간주됨

<br>

### 2) 변수 변환 방법

#### (1) 범주형 변환

연속형 변수를 분석결과의 명료성 및 정확성을 위해 범주형으로 변환

> '소득이 100만원 증가할 때마다 사교육비의 지출이 10만원 증가한다' 보다, '상위 10% 소득가정의 사교육비 지출이 하위 10%보다 10배 많다' 가 이해하기 쉽다

- 소득의 연속적 데이터를 그대로 사용하기보다 순위형 데이터로 범주를 나눈다

#### (2) 정규화

분석의 정확성을 위해 연속형(이산형) 데이터를 바로 사용하지 않고 정규화를 거치는 경우가 많다 (특히, 데이터의 스케일 차이가 심한 경우)

- 정규화의 종류
  - 일반 정규화

    - 수치로된 값들을 여러 개 사용할 때 각 수치의 범위가 다르면 이를 같은 범위로 변환해 사용하는 방법

      

  - 최소-최대 정규화(Min-Max Normalization)

    - 가장 일반적인 정규화 방법

    - 모든 feature에 대해 최소값 0, 최대값 1 사이로 변환

    - 단점은 **이상치의 영향을 많이 받는다**
      $$
      \text{Min- Max Normalized}~ X = \frac{X - \text{Min}}{\text{Max} - \text{Min}}
      $$
      

  - Z-점수(Z-Score) 정규화

    - **이상치 문제를 피할 수 있는 정규화** 방법으로, 데이터가 **표준정규분포(가우시안분포)가 되도록** 값을 바꿔준다
    - 데이터의 평균값을 0으로, 표준편차를 1로 변환. 

    $$
    Z = \frac{X - \mu}{\sigma}
    $$

    - 분모에 해당하는 표준편차가 클수록 정규화되는 Z 값이 0에 가까워짐

    - 이상치를 잘 처리하지만 정확히 동일한 척도로 정규화된 데이터를 생성하지 않는다 (일반 정규화와 최소최대 정규화에 비해)

      

  - 로그 변환(Log Transformation)

    - 수치값에 로그를 취한 값을 사용
    - **로그를 취하면 그 분포가 정규분포에 가까워질 때 사용** - 로그정규분포(Log-normal Distribution)

    $$
    X \sim ln(X)
    $$

    - 로그변환을 사용하는 경우

      - 데이터 분포가 **우측**으로 치우친 경우

        ex) 국가별 수출액, 사람의 통증 정도 수치화, 주식가격 변동성 분석 등

        

  - 역수 변환(Inverse Transformation)

    - **역수를 취했을 때 오히려 선형성을 가지게 되어 의미를 해석하기 쉬워지는 경우**에 사용
    - 데이터가 **극단적인 우측**으로 치우친 경우에 사용

    $$
    X \sim 1 / X
    $$

    

  - 지수 변환(Power Transformation)

    - **지수변환을 했을 때 오히려 선형성을 가지게 되어 의미를 해석하기 쉬워지는 경우**에 사용

    - 데이터가 **좌측**으로 치우친 경우 사용
      $$
      X \sim X^n
      $$
      

  - 제곱근 변환(Square Root Transformation)

    - **제곱근변환을 했을 때 오히려 선형성을 가지게 되어 의미를 해석하기 쉬워지는 경우**에 사용

    - 데이터가 **우측**으로 약간 치우친 경우 사용
      $$
      X \sim \sqrt{X}
      $$
      

- 데이터 분포 형태별 정규화 방법

  |      기존 분포       | 정규화 변환식 |
  | :------------------: | :-----------: |
  |     좌로 치우침      |     $X^3$     |
  |   좌로 약간 치우침   |     $X^2$     |
  |   우로 약간 치우침   |  $\sqrt{X}$   |
  |     우로 치우침      |    $ln(X)$    |
  | 극단적인 우로 치우침 | $\frac{1}{X}$ |

  
  - 데이터의 정규성 검정은 데이터 분포 형태를 눈으로 확인할 수도 있지만 샤피로테스트(Shapiro Test) 혹은 큐큐플롯(Q-Q Plot)을 이용해 확인 가능하며, 결과에 따라 적당한 변수변환식을 사용해 정규분포 형태로 변환해준다

<br><br>

## 5. 불균형 데이터 처리

특정 데이터에서 각 클래스(범주형 반응변수)가 갖고있는 데이터의 양에 차이가 큰 경우 클래스 불균형이 있다고 말한다.



### 1) 불균형데이터의 문제점

데이터 클래스 비율이 너무 차이가 나면(Highly-imbalanced Data) 단순히 우세한 클래스를 택하는 모델의 정확도가 높아지게 된다.

즉, **정확도(accuracy)가 높아도 데이터 개수가 적은 클래스의 재현율(recall-rate)이 급격히 작아지는 현상**이 발생한다.
$$
\text{정확도(Accuracy)} = \frac{TP + TN}{TP + TN + FP + FN}\\\\

\text{재현율(Recall)} = \frac{TP}{TP + FN}
$$

### 2) 불균형 데이터의 처리

#### (1) 가중치 균형 방법(Weighted Balancing)

모델에서 손실(loss)을 계산할 때 **특정 클래스의 데이터에 더 큰 loss 값을 갖도록** 하는 방법.

- 고정 비율 이용

  - 클래스의 비율에 따라 가중치를 두는 방법. 

    ex) 각 클래스의 데이터 비율이 1:5라면 가중치를 5:1로 줌으로써 적은 샘플 수를 가진 클래스를 전체 loss에 동일하게 기여하도록 만듦

- 최적 비율 이용

  - 분야와 최종 성능을 고려해 가중치 비율의 최적값을 찾아가는 방법

    <br>

#### (2) 언더샘플링(Undersampling)과 오버샘플링(Oversampling)

언더샘플링과 오버샘플링을 사용해 데이터 비율을 맞추면 정밀도(precision)이 향상됨

- 언더샘플링
  - 대표클래스(Majority Class)에서 일부만 사용, 소수클래스(Minority Class)는 최대한 많이 사용
  - 이 때 언더샘플링된 대표클래스 데이터는 원본 데이터와 비교해 대표성을 가져야 함
- 오버샘플링
  - 소수 클래스 데이터를 복사해 대표클래스 수만큼 만들어줌
  - 똑같은 데이터를 복사하는 것이므로 추가된 데이터는 기존데이터와 같은 성질을 갖게 됨



