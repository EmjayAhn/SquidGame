# 01 데이터 정제

## 1. 데이터 이해

### 1) 데이터 관련 정의

- 데이터(Data): 이론을 세우는 기초가 되는 사실 또는 자료. 컴퓨터와 연관되어 프로그램을 운용할 수 있는 형태로 기호화/수치화된 자료.
- 단위(Unit): 관찰되는 항목 또는 대상
- 관측값(Observation): 각 조사단위별 기록정보 또는 특성
- 변수(Variable): 각 단위에서 측정된 특성
- 원자료(Raw Data): 표본에서 조사된 최초의 자료

<br>

### 2) 데이터의 종류

- 단변량 자료(Univariate Data): 자료의 특성을 대표하는 특성변수가 하나인 자료
- 다변량 자료(Multivariate Data): 자료의 특성을 대표하는 특성변수가 둘 이상인 자료
- 질적 자료(Qualitative Data): 정성적 또는 범주형 자료
  - 명목 자료(Nominal Data): 수치 또는 기호로 분류되는 자료 (ex. 지역번호)
  - 서열 자료(Ordinal Data): 명목자료와 비슷하지만 수치나 기호가 서열을 나타내는 자료 (ex. 랭킹)
- 수치 자료(Quantitative Data): 정량적 또는 연속형 자료 
  - 구간 자료(Interval Data): 명목자료, 서열자료의 의미를 포함하면서 숫자로 표현된 변수간의 관계가 산술적인 의미를 갖는 자료 (ex. 온도)
  - 비율 자료(Ratio Data): 명목자료, 서열자료, 구간자료의 의미를 다 가지면서 수치화된 변수에 비율의 개념을 도입할 수 있는 자료 (ex. 무게)

- 시계열 자료(Time Series Data): 일정한 시간동안 수집된, 시간개념이 포함된 자료 (ex. 일별 주가)
- 횡적 자료(Cross Sectional Data): 특정 단일시점에서 여러 대상으로부터 수집된 자료
- 종적 자료(Longtitudinal Data): 시계열자료와 횡적자료의 결합으로, 여러 개체를 여러 시점에서 수집한 자료

> 자료 특성에 따라 자료로부터 결과를 도출하기 위한 방법론이 달라지기 때문에 다양한 종류의 데이터를 파악하는 것이 매우 중요하다.

<br>

### 3) 데이터의 정제

수집된 데이터를 분석에 용이하도록 추출, 통합하고 각 분석 기법에 맞게 다듬는 과정이다.

정제과정을 거치지 않은 데이터를 사용하면 데이터 구성의 일관석이 없어져 분석의 처리에 어려움이 발생할 수 있고, 도출된 결과의 신뢰성도 저하된다.

- 데이터 정제 과정(Processing)

  - 다양한 매체로부터 데이터를 **수집**, 원하는 형태로 **변환**, 원하는 장소에 **저장**, 저장된 데이터의 **품질확인**, 필요한 시기와 목적에 따라 데이터를 원활히 사용할 수 있도록 **관리**하는 과정이 필요하다.
  - 데이터를 처음 수집하면 정형보다 비정형 데이터(Unstructured Data)인 경우가 많다. 이를 구조화된 정형 데이터(Structured Data)로 변환하고, 변환된 데이터에서 결측치나 오류 등을 해결하는 과정을 거친다.
  - 기존에 수집했던 데이터와 비교분석이 필요한 경우 레거시(Legacy Data)와의 통합, 변환과정이 필요할 수 있다.

  | 구분            | 수행내용                                                     | 프로세스                   |
  | --------------- | ------------------------------------------------------------ | -------------------------- |
  | 데이터 **수집** | - 데이터의 수집방법 및 정책 결정<br />- 데이터 수집경로 구조화<br />- 데이터 집계(Aggregation)<br />- 데이터 저장소 결정 | 전처리(preprocessing) 포함 |
  | 데이터 **변환** | - 데이터 유형의 변화 및 분석가능한 형태로 가공<br />- ETL(Extraction, Transformation, Loading)<br />- 일반화(Generalization)<br />- 정규화(Normalization) |                            |
  | 데이터 **교정** | - 결측치, 이상치(Outlier), 노이즈 처리<br />- 비정형데이터 수집시 필수 |                            |
  | 데이터 **통합** | - 데이터분석이 용이하도록 기존 또는 유사데이터와의 연계 통합<br />- 레거시 데이터(Legacy Data)와 함께 분석이 필요할 경우 수행 |                            |

  

- 데이터 정제의 전처리, 후처리
  - 전처리(Pre Processing): 데이터 저장 전의 처리과정으로, 대상 데이터와 수집방식 결정 및 저장소를 선정
  - 후처리(Post Processing): 데이터 저장 후의 처리과정으로 저장데이터의 품질관리 등의 과정을 포함

<br>

## 2. 데이터 결측값 처리

결측값(Missing Data)이란 데이터가 없음을 의미한다.

결측치를 임의로 제거할 경우 분석데이터가 손실되어 분석에 필요한 수준의 데이터 수집에 실패할 수 있다.

결측치를 임의로 대체할 경우 데이터의 편향(bias)이 발생하여 분석결과의 신뢰성이 떨어진다.

결측치 처리는 데이터에 기반한 다양한 방법으로 처리할 수 있다.

<br>

### 1) 결측데이터의 종류

- 완전 무작위 결측(MCAR: Missing Completely At Random): 결측데이터가 다른변수와 아무런 연관이 없는 경우

- 무작위 결측(MAR: Missing At Random): 결측데이터가 다른 변수와 연관되어있지만 그 결측된 변수 자체와는 관련이 없는 경우

- 비무작위결측(NMAR: Not Missing At Random): MCAR 또는 MAR이 아닌 결측데이터로, 결측값이 결측여부(이유)와 관련이 있는 경우

  

  예시)

  - 나이, 성별, 체중 변수에 대한 데이터 수집을 가정할 때,
    - 세 변수와 관계없이 무응답 등으로 데이터가 단순 누락된 경우 -> 완전 무작위 결측(MCAR)
    - 여성은 체중공개를 꺼리는 경우가 많아 체중 데이터가 누락될 가능성이 성별 변수에 의존 -> 무작위 결측(MAR)
    - 체중이 많이 나가는 사람들은 체중공개를 꺼려 체중 데이터가 누락될 가능성이 체중 변수 자체에 의존 -> 비무작위 결측(NMAR)

<br>

### 2) 결측값 유형의 분석 및 대치

결측데이터를 처리할 때는 효율성, 복잡성, 편향 등의 문제를 고려하여 상황/목적에 따라 다르게 처리해야 한다.

- 단순 대치법(Simple Imputation)

  기본적으로 결측치를 MCAR 나 MAR로 판단하고 처리하는 방법

  - 완전 분석(Complete Analysis): 결측치는 완전히 제외하고 분석을 수행. 가장 용이한 처리방법이나 효율성 상실 및 통계적 추론의 타당성 문제가 발생.
  - 평균 대치법(Mean Imputation): 데이터의 평균값으로 결측치를 대치. 효율성 측면에서 장점이 있으나 통계량의 **표준오차가 과소추정**되는 단점이 있음.
  - 회귀 대치법(Regression Imputation): 회귀분석에 의한 예측치로 결측치를 대치. 
  - 단순확률 대치법(Single Stochastic Imputation): 평균 대치법에서 추정량 **표준오차의 과소추정이 발생하는 점을 보완**한 방법으로 Hot-deck 방법이라고도 함. **확률 추출에 의해 전체 데이터 중 무작위로 값을 대치**.
  - 최근접 대치법(Nearest-Neighbor Imputation): 전체표본을 몇개의 대체군으로 분류해 각 군집에서의 응답자료를 순서대로 정리한 후 결측값 바로 이전의 응답을 결측치로 대치. 응답값이 여러번 사용될 수 있다는 점이 단점.

- 다중 대치법(Multiple Imputation) ([참고자료](https://www.jpmph.org/upload/pdf/jpmph-37-3-209.pdf))

  단순대치법을 복수 시행하여 통계적 효율성 및 일치성 문제를 보완할 수 있는 방법. n개의 단순대치를 통해 n개의 새로운 자료를 만들어 분석을 시행하고, 그 결과로 얻어진 통계량에 대해 통계량 및 분산 결합을 통해 통합하는 방법

  - 1단계 - 대치 단계(Imputation): n회의 단순대치 시행으로 결측을 대치한 데이터셋 n개 생성 
    - ex) 원래 데이터셋이 100개의 데이터케이스를 포함한다면 5회 단순대치를 반복하면 총 600개의 데이터케이스가 생기는 것 
    - n이 크면 클수록 바람직한 결과를 도출할 수 있지만 분석 소요시간이 너무 길어지므로 일반적으로 3~5개로 설정.
  - 2단계 - 분석 단계(Analysis): n개의 데이터셋 각각에 대한 표준통계분석
    - n개의 추정량과 분산 값 도출
  - 3단계 - 결합 단계(Combination): n개의 분석결과에 대한 통계적 결합을 통해 결과 도출

<br>

## 3. 데이터 이상값 처리

이상치(이상값, Outlier)란 데이터의 전체적인 패턴, 즉 정상적인 범주에서 벗어난 값을 의미한다. 

### 1) 이상치의 종류 및 발생원인

- 이상치의 종류
  - 단변수 이상치(Univariate Outlier): 하나의 데이터 분포에서 발생하는 이상치
  - 다변수 이상치(Multivariate Outlier): 복수의 연결된 데이터 분포공간에서 발생하는 이상치
- 이상치의 발생원인
  - 비자연적 이상치 발생(Artificial/Non-Natural Outlier)
    - 입력 실수(Data Entry Error): 데이터 수집과정에서 입력 실수 등으로 발생하는 에러
    - 측정 오류(Measurement Error): 데이터 측정 중에 측정기 오작동 등으로 발생하는 에러
    - 실험 오류(Experimental Error): 실험과정 중 실험환경에서 야기된 문제들로 발생하는 에러
    - 의도적 이상치(Intentional Outlier): 의도적으로 키를 높게 기입하는 등 의도가 포함된 이상치
    - 자료처리 오류(Data Processing Error): 여러개의 데이터셋에서 데이터를 추출, 통합하는 등 분석 전 전처리과정에서 발생하는 에러
    - 표본 오류(Sampling Error): 모집단에서 표본을 추출하는 과정에서 편향이 발생한 경우
  - 자연적 이상치(Natural Outlier)
    - 위 오류들이 아닌 모든 이상치들에 해당

### 2) 이상치의 문제점

- 기초 통계분석결과의 신뢰도 저하
  - 이상치는 평균, 분산 등 기초적인 통계치에 영향을 준다. (*단, 중앙값에는 영향이 적다*)

- 기초 통계에 기반한 고급 통계분석의 신뢰성 저하
  - 검정, 추정 등의 분석, 회귀분석 등에 영향을 준다
  - 특히 이상치가 비무작위성을 가진 분포로 나타나게 되면 **데이터의 정상성(Normality)이 감소**하며 이는 데이터 자체의 신뢰성 저하로 연결된다

### 3) 이상치의 탐지

변수가 다변량인지 단변량인지, 모수적(Parametric)인지 비모수적(Non-Parametric)인지[^myfootnote] 등을 고려해 다양한 방식으로 이상치를 탐지해볼 수 있다.

- 시각화를 통한 방법 (비모수적, 단변량 데이터의 경우)

  - 상자수염그림(상자그림, Box Plot), 줄기-잎 그림(Stem and Leaf Diagram)
  - 산점도 그림(Scatter Plot)

- Z-score를 통한 방법 (모수적 단변량 또는 저변량의 경우)

  - 정규화를 통해 특정 threshold를 벗어난 경우를 이상치로 판별

  $$
  z_i = \frac{x_i - \mu}{\sigma}, ~~~~~~~~~~~ \mu : \text{평균} ~~~~ \sigma : \text{표준편차}\\
  |z_i| > z_{threshold}
  $$

  - 통상적 threshhold는 1$\sigma$ 사이(전체의 68.27%), 2$\sigma$ 사이(전체의 95.45%), 3$\sigma$​ 사이(전체의 99.73%) 등을 사용

- 밀도 기반 클러스터링 방법 (DBSCAN: Density Based Spatial Clustering of Application with Noise)

  - 비모수적 다변량 데이터의 경우 군집간 밀도를 이용해 특정 거리 내의 데이터 수가 지정 개수 이상이면 군집으로 정의해 군집에서 먼 거리에 있는 데이터를 이상치로 간주

- 고립 의사나무 방법 (Isolation Forest)
  - 비모수적 다변량 데이터의 경우 의사결정나무(Decision Tree) 기반으로 정상치의 단말 노드(Terminal Node)보다 이상치의 노드에 이르는 길이(Path Length)가 더 짧은 성질을 이용하는 방법

<br><br>

[^myfootnote]: 데이터가 정규분포를 따르며, 표본수가 충분하며, 수집된 데이터의 환경이 모두 같은 경우 모수, 데이터가 정규분포가 아니며 표본 수가 적거나 부족하고 데이터가 서로 독립적인 경우 비모수라고 한다. 단순히 생각하면 모수는 연속된 값, 비모수는 연속되지 않은 값이라고 이해해도 된다. 매출과 같은 연속적인 데이터의 경우 정규분포를 따를 수 있지만, 순위와 같은 비연속적인 데이터는 정규분포를 따를 수 없기 때문에 각 경우에는 분석 및 검정방법을 다르게 사용해야 한다. 모수적 검정기법은 모집단으로부터 추출한 표본 간 평균 차이를 중심으로 분석한다. 단순한 평균비교부터 분산분석 등 다양한 분석방법이 있다. 하지만 비모수적 검정 기법은 평균이 아닌 서열 또는 특정 기준값(일반적으로 중앙값)을 중심으로 한 부호에 무게를 두고 분석한다. 따라서 비모수적 검정기법에서는 데이터값이 극단적이더라도 영향을 덜 받는다. 연속확률분포에서는 평균의 차이가 매우 중요하지만 이산확률분포에서는 평균 차이와 관계없이 순위차이를 기준으로 분석이 이루어지기 때문이다. 일반적으로는 데이터가 충분하고 정규분포를 따른다고 가정하고 모수적인 검정을 수행하게 된다. [참조](https://brunch.co.kr/@plusstar/183)
